# Summer of Science 2024: Natural Language Processing
### Mentee: Abhi Jain
### Mentor: Nilesh Choudhary

## Overview
This repository contains the reading project report for the Summer of Science 2024: Natural Language Processing project. The primary focus of this project is to explore various aspects of Natural Language Processing (NLP), covering both foundational and advanced concepts. 
  
# Table of Contents
## Introduction to NLP and Text Preprocessing
- Overview of NLP
- Tokenization, Stop-word Removal, Stemming and Lemmatization, Punctuation Removal

## Text Representation
- One-Hot Encoding, Bag of Words, n-gram Models
- TF-IDF, Word Embeddings (Word2Vec)

## Deep Learning for NLP
- Neural Networks (Feedforward Networks, Training, Optimization)
- Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs)

## Attention Mechanism, Seq2seq, and Encoder-Decoder Models
- Encoder-Decoder Models, Seq2Seq using LSTMs
- Attention Mechanism

## Transformers
- Self-Attention, Multi-Head Attention, Transformer Blocks
- Language Modeling with Transformers

## LLMs with Transformers
- Applying Transformer Models to NLP Tasks
- Sampling Techniques (Top-k, Nucleus, Temperature Sampling)
- Practical Problems with LLMs

## Fine-tuning and Bi-directional Transformers
- Bi-directional Encoder Models, Training Regimes, Contextual Embeddings
- Sequence Classification, Pair-wise Sequence Classification

## NLP Pipeline
- Data Acquisition, Text Extraction, Preprocessing
- Model Building and Deployment
